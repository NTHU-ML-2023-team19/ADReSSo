{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4k0uw-d0vTA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Ni5e2vx04R_"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import XLNetTokenizer, TFXLNetModel\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qTk7Wxjp14JW"
      },
      "outputs": [],
      "source": [
        "transcript = pd.read_csv(\"ASR.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xjJbCbmz1Ep4"
      },
      "outputs": [],
      "source": [
        "models = {'roberta':(RobertaTokenizer,'roberta-large',TFRobertaModel),\n",
        "          'bert':(BertTokenizer, 'bert-large-uncased', TFBertModel),\n",
        "          'xlnet':(XLNetTokenizer, 'xlnet-large-cased', TFXLNetModel)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "COL3gI1G1HbI"
      },
      "outputs": [],
      "source": [
        "tokenizer, model_type, model_name = models['roberta']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UNomHjBv1MO3"
      },
      "outputs": [],
      "source": [
        "def make_inputs(tokenizer, model_type, serie, max_len= 128):\n",
        "\n",
        "    tokenizer = tokenizer.from_pretrained(model_type, lowercase=True )\n",
        "    tokenized_data = [tokenizer.encode_plus(text, max_length=max_len,\n",
        "                                            padding='max_length',\n",
        "                                            add_special_tokens=True,\n",
        "                                            truncation = True) for text in serie]\n",
        "\n",
        "\n",
        "    input_ids = np.array([text['input_ids'] for text in tokenized_data])\n",
        "    attention_mask = np.array([text['attention_mask'] for text in tokenized_data])\n",
        "\n",
        "    return input_ids, attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d8N0mjB1OhM"
      },
      "outputs": [],
      "source": [
        "input_ids_train, attention_mask_train = \\\n",
        "make_inputs(tokenizer, model_type, transcript[\"transcription\"], max_len= 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lyZ3oP1F1RRU"
      },
      "outputs": [],
      "source": [
        "##### TPU or no TPU\n",
        "def init_model(model_name, model_type, num_labels, Tpu = 'on', max_len = 128):\n",
        "# ------------------------------------------------ with TPU --------------------------------------------------------------#\n",
        "    if Tpu == 'on':\n",
        "        # a few lines of code to get our tpu started and our data distributed on it\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        # print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "        with strategy.scope():\n",
        "\n",
        "            model_ = model_name.from_pretrained(model_type)\n",
        "            # inputs\n",
        "            input_ids = tf.keras.Input(shape = (max_len, ), dtype = 'int32')\n",
        "            attention_masks = tf.keras.Input(shape = (max_len,), dtype = 'int32')\n",
        "\n",
        "            outputs = model_([input_ids, attention_masks])\n",
        "\n",
        "            if 'xlnet' in model_type:\n",
        "                # cls is the last token in xlnet tokenization\n",
        "                outputs = outputs[0]\n",
        "                cls_output = tf.squeeze(outputs[:, -1:, :], axis=1)\n",
        "            else:\n",
        "                cls_output = outputs[1]\n",
        "\n",
        "            final_output = tf.keras.layers.Dense(num_labels, activation = 'softmax')(cls_output)\n",
        "            model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = final_output)\n",
        "            model.compile(optimizer = Adam(lr = 1e-5), loss = 'categorical_crossentropy',\n",
        "                        metrics = ['accuracy'])\n",
        "# ------------------------------------------------ without TPU --------------------------------------------------------------#\n",
        "    else:\n",
        "        model_ = model_name.from_pretrained(model_type)\n",
        "        # inputs\n",
        "        input_ids = tf.keras.Input(shape = (max_len, ), dtype = 'int32')\n",
        "        attention_masks = tf.keras.Input(shape = (max_len,), dtype = 'int32')\n",
        "\n",
        "        outputs = model_([input_ids, attention_masks])\n",
        "\n",
        "        if 'xlnet' in model_type:\n",
        "            # cls is the last token in xlnet tokenization\n",
        "            outputs = outputs[0]\n",
        "            cls_output = tf.squeeze(outputs[:, -1:, :], axis=1)\n",
        "        else:\n",
        "            cls_output = outputs[1]\n",
        "\n",
        "\n",
        "        final_output = tf.keras.layers.Dense(num_labels, activation = 'softmax')(cls_output)\n",
        "\n",
        "        model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = final_output)\n",
        "\n",
        "        model.compile(optimizer = Adam(lr = 1e-5), loss = 'categorical_crossentropy',\n",
        "                    metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrvgTLii1UXt"
      },
      "outputs": [],
      "source": [
        "model = init_model(model_name, model_type, num_labels = 2, Tpu = 'off', max_len = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sLNAGqDs1Zz-"
      },
      "outputs": [],
      "source": [
        "train_y = tf.keras.utils.to_categorical(transcript[\"label\"], num_classes=2)\n",
        "del transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeIrT8Ct1ePe"
      },
      "outputs": [],
      "source": [
        "model.fit([input_ids_train, attention_mask_train], train_y,\n",
        "          validation_split=0.25, epochs = 10, batch_size = 8,\n",
        "          shuffle = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
